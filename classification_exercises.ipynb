{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88991e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pydataset import data\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Obtain IRIS dataset from data\n",
    "df_iris = data('iris')\n",
    "df_iris.head()\n",
    "\n",
    "# can use sns version also..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a PRINT first three rows\n",
    "# df_iris.head(3) or\n",
    "df_iris.loc[0:3,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b Print the Shape\n",
    "df_iris.shape # 150 rows by 5 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4376e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4c print col names\n",
    "df_iris.columns  # list(df_iris.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d060b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4d print data types for each col\n",
    "df_iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4e print summary stats for each numeric variable\n",
    "df_iris.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd927094",
   "metadata": {},
   "source": [
    "## DF from Google Sheet\n",
    "- replace '/edit' with '/export'  \n",
    "- add 'format=csv' to the beginning of the query string\n",
    "- ex csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5dc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'    \n",
    "\n",
    "# csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "\n",
    "# df_googlesheet = pd.read_csv(csv_export_url)\n",
    "# df_googlesheet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a read first 3 lines from titanic data\n",
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41719bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet = pd.read_csv(csv_export_url)\n",
    "df_googlesheet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d8e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b print the number of rows and cols\n",
    "df_googlesheet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5c print col names\n",
    "df_googlesheet.columns # or .to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5d print dtypes\n",
    "df_googlesheet.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0480b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5e print summary statistics for each numeric variable\n",
    "df_googlesheet.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e56d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet['PassengerId'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd51478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df_googlesheet.columns:\n",
    "#     if df_googlesheet[col].dtypes == 'object':\n",
    "#         #if the string literal == adtype of object\n",
    "#         print(f'{col} has {df_googlesheet.nunique()} unique values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5f print unique values for each of the categorical variables\n",
    "categories = [df_googlesheet.Survived.unique(),\n",
    "             df_googlesheet.Pclass.unique(),\n",
    "              df_googlesheet.SibSp.unique(),\n",
    "             df_googlesheet.Parch.unique(),\n",
    "             ]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32031225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_googlesheet['Embarked'].isna()\n",
    "df_googlesheet['Embarked'].value_counts(dropna=False)\n",
    "df_googlesheet['Embarked'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dadcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 download file into excel \n",
    "df_excel = pd.read_excel('train.xlsx')  #sheet_name='ddfs' is an arg to add\n",
    "df_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce01d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a assign first 100 rows to sample\n",
    "df_excel_sample = df_excel.head(100)\n",
    "df_excel_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a2c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b print number of rows from original df\n",
    "df_excel.index # df_excel.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c print first 5 col names\n",
    "df_excel_sample.columns[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9492e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS MUCH EASIER TO GRAB dytpe of X!!!\n",
    "\n",
    "# 6d print the column names that have dtype of object\n",
    "df_excel_sample.select_dtypes(include='object').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ac182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_sample.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_sample.PassengerId.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb79c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_excel_sample.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ccf4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6e compute range for each numeric value\n",
    "df_excel_sample.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_descr_T = df_excel_sample.describe().T\n",
    "df_excel_descr_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b19b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel_descr_T['range'] = (df_excel_descr_T['max'] - df_excel_descr_T['min'])\n",
    "df_excel_descr_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire as acq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 this function call will also create a .csv (which will be ignored by GIT)\n",
    "# 4 functionality has been added already\n",
    "acq.get_titanic_data('titanic_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 this function call will also create a .csv (which will be ignored by GIT)\n",
    "# 4 functionality has been added already\n",
    "acq.get_iris_data('iris_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd59fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 this function call will also create a .csv (which will be ignored by GIT)\n",
    "# 4 functionality has been added already\n",
    "acq.get_telco_data('telco_churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5cb406",
   "metadata": {},
   "source": [
    "## DATA PREPARE EX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4af434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Using IRIS data use acquire.py to load\n",
    "import acquire as acq\n",
    "iris_df = acq.get_iris_data('iris_db')\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93992f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 drop columns\n",
    "iris_df.drop(columns=['species_id','measurement_id'],inplace=True)\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c7801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Rename\n",
    "iris_df.rename(columns={'species_name':'species'},inplace=True)\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Create Dummy variables for species name and concat into iris_df\n",
    "iris_df = pd.concat(\n",
    "                    [iris_df,\n",
    "                     pd.get_dummies(iris_df['species'], drop_first=True)],\n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ba179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire as acq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare as prp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Create a function which takes in df and outputs the above transformations\n",
    "\n",
    "def prep_iris(df):\n",
    "    '''\n",
    "    prep iris will take in a single pandas dataframe\n",
    "    and drop columns\n",
    "    and aaddress NULLS\n",
    "    and encoding cats\n",
    "    '''\n",
    "    #df = acq.get_iris_data('iris_db')\n",
    "    df = df.drop(columns=['species_id','measurement_id'])\n",
    "    df = df.rename(columns={'species_name':'species'})\n",
    "    df = pd.concat([df,pd.get_dummies(df['species'],drop_first=True)],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b65248e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db = acq.get_iris_data('iris_db')\n",
    "type(db)\n",
    "db.head()\n",
    "#db = db.rename(columns={'species_id':'oneone'})\n",
    "# db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ed6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = prp.prep_iris(db)\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c446e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5182b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e043e6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b1ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6debc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad20717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d38e5d50",
   "metadata": {},
   "source": [
    "## USING TITANIC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e023c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# # make the thing...\n",
    "# #titanic\n",
    "# my_imputer = SimpleImputer(strategy='mean')\n",
    "# #fit the thing...\n",
    "# my_imputer.fit(titanic_df['age'])\n",
    "# #use the thng\n",
    "# titanic['age'] = my_imputer.transform(titanic_df['age'])\n",
    "\n",
    "# then use the imputer to fill in missingness for TRAIN data initialy...\n",
    "# then add that imputation process to validate and TEST data...\n",
    "# ie your IMPUTATION ASSUMPTIONS should be formulated based on TRAIN data, not WHOLE data or any part of TEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421340ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load titanic via acquire.py\n",
    "df = acq.get_titanic_data('titanic_db')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_titanic(df):\n",
    "    '''\n",
    "    Accepts a df and drops unneeded columns, imputes missing values, and encodes cats\n",
    "    '''\n",
    "    #drop out any redundant, excessively empty, or bad columns\n",
    "    df = df.drop(columns=['passenger_id','embarked','deck','class'])\n",
    "    # impute average age and most common embark_town:\n",
    "    df['age'] = df['age'].fillna(df.age.mean())\n",
    "    df['embark_town'] = df['embark_town'].fillna('Southampton')\n",
    "    # encode categorical values:\n",
    "    df = pd.concat(\n",
    "    [df, pd.get_dummies(df[['sex', 'embark_town']],\n",
    "                        drop_first=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740de2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_titanic(acq.get_titanic_data('titanic_db'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbd720",
   "metadata": {},
   "outputs": [],
   "source": [
    "prp.prep_titanic(df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381a813",
   "metadata": {},
   "source": [
    "## USING TELCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load telco via acquire.py\n",
    "df = acq.get_telco_data('telco_churn')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_telco(df):\n",
    "    '''\n",
    "    Accepts a df and drops unneeded columns, imputes missing values, and encodes cats\n",
    "    '''\n",
    "    \n",
    "    #drop out any redundant, excessively empty, or bad columns\n",
    "    df = df.drop(columns=['payment_type_id','internet_service_type_id','contract_type_id'])\n",
    "\n",
    "    # encode categorical values:\n",
    "    df = pd.concat(\n",
    "    [df, pd.get_dummies(df[['gender', 'senior_citizen','partner','dependents','tech_support','streaming_tv',\n",
    "                           'streaming_movies','paperless_billing','churn','contract_type',\n",
    "                            'internet_service_type','payment_type']],\n",
    "                        drop_first=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b92c00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = prep_telco(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f021fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POTENTIAl issues with telco...appear to be no NULLS\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34269546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()[df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # which are cats?\n",
    "# dont_encode = []\n",
    "# encode = []\n",
    "# binaries = []\n",
    "\n",
    "# # for col in df.columns:\n",
    "# #     if df[col].dtype == 'O':\n",
    "#           if telco[col].nunique() > 5:\n",
    "#              dont_encode.append(col)\n",
    "#           elif telco[col].nunique() > 5:\n",
    "#              binaries.append(col)\n",
    "#           else:\n",
    "#              encode.append[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c657b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af783455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.map({'Female':1,\n",
    "              'Male':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e212f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df[['gender']],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262be49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae478b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.customer_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490674f",
   "metadata": {},
   "source": [
    "## SPLIT the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d258f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67e3f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_val,test = train_test_split(df,train_size=0.8,random_state=2013,stratify=df['churn_Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200dfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8144072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate = train_test_split(train_val, train_size=0.7,random_state=2013,\n",
    "                                   stratify=train_val['churn_Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e08b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 this function also exists in prepare.py as prp\n",
    "def split_data(df, target):\n",
    "    '''\n",
    "    split_data will split data into train,val,test based on \n",
    "    the values present in a cleaned version of df.  By default, data will be stratified\n",
    "    by the target label\n",
    "    \n",
    "    '''\n",
    "    train_val, test = train_test_split(df,train_size=0.8,random_state=2013,\n",
    "                                   stratify=df[target])\n",
    "    train, validate = train_test_split(train_val,train_size=0.7,random_state=2013,\n",
    "                                   stratify=train_val[target])\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06865d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare as prp\n",
    "df = iris_df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e95d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 run prp.split_data on the iris dataset.  This should yield train_iris,validate_iris, and test_iris\n",
    "prp.split_data(df,'species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris,validate_iris,test_iris = prp.split_data(df,'species')\n",
    "train_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed429fe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3 run prp.split_data on the Titanic dataset.  This should yield train_titanic,validate_titanic, and test_titanic\n",
    "df = prep_titanic(acq.get_titanic_data('titanic_db'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prp.split_data(df,'survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titanic,validate_titanic,test_titanic = prp.split_data(df,'survived')\n",
    "train_titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2244893",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_telco(acq.get_telco_data('telco_churn'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918777c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 run prp.split_data on the Telco dataset.  This should yield train_telco,validate_telco, and test_telco\n",
    "train_telco,validate_telco,test_telco = prp.split_data(df,'churn_Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bcd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_telco.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e537920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_telco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb20fd",
   "metadata": {},
   "source": [
    "### EXPLORE exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fd55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 using iris_db\n",
    "# 1 Using IRIS data use acquire.py to load\n",
    "import acquire as acq\n",
    "iris = acq.get_iris_data('iris_db')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ef214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare as prp\n",
    "iris = prp.prep_iris(iris)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d14e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to drop the one-hot encoding of the target variable...will lead to data leakage later on if we process\n",
    "# df as it is with target being encoded\n",
    "iris = iris.drop(columns=['versicolor','virginica'])\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21862cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris,validate_iris,test_iris = prp.split_data(iris,'species')\n",
    "train_iris.shape,validate_iris.shape,test_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86200523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pydataset import data\n",
    "import env\n",
    "import acquire as acq\n",
    "import prepare as prp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3500c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 UNIVARIATE STATS\n",
    "train_iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris['sepal_width'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f807b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Cats from Nums\n",
    "cat_cols,num_cols = [],[]\n",
    "for col in train_iris.columns:\n",
    "    if train_iris[col].dtype == 'O':\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        if train_iris[col].nunique() < 10:\n",
    "            cat_cols.append(col)\n",
    "        else:\n",
    "            num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7eae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each Num col, show hist/boxplot/describe()\n",
    "for col in num_cols:\n",
    "    sns.histplot(data=train_iris, x=col)\n",
    "    plt.show()\n",
    "    sns.boxplot(data=train_iris, x=col)\n",
    "    plt.show()\n",
    "    print(train_iris[col].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fa4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a10f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 For each Cat, create freq table and a bar plot\n",
    "for col in cat_cols:\n",
    "    print(f'Univariate assess feature {col}:')\n",
    "    sns.countplot(data=train_iris,x=col)\n",
    "    plt.show()\n",
    "    print(\n",
    "    pd.concat([train_iris[col].value_counts(),\n",
    "               train_iris[col].value_counts(normalize=True)],\n",
    "              axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris.shape\n",
    "test_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c36f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TAKEAwAYS\n",
    "\n",
    "-All 3 SPECIES are equally represented in TRAINING SET\n",
    "\n",
    "-Asssumption of NOrmality applies to : Sepal Length,Sepal Width,\n",
    "\n",
    "-Petal Width and Petal Length are bi-modal, however the greater leaf surface areas are NORMALLY DISTRIBUTED\n",
    " in the case of Petal Length\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e166df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOOK at Bivariate Comparisons\n",
    "# check each num with target and add a HORIZONTAL MEAN LINE\n",
    "for col in num_cols:\n",
    "    sns.boxplot(data=train_iris,x='species', y=col)\n",
    "    plt.axhline(train_iris[col].mean(),color=\"red\", linestyle=\"dashed\",linewidth=5)\n",
    "    plt.title(f'species based on {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56249ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate descriptive stats for each num_col all independent variable cols are num_cols\n",
    "train_iris.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48344742",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris['sepal_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    print(train_iris[col].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris.sepal_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d792d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare virginica and versicolor mean PETL_WIDTH using Mann-Whitney\n",
    "virginica = train_iris[train_iris.species=='virginica'].petal_width\n",
    "versicolor = train_iris[train_iris.species=='versicolor'].petal_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7668df",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "stat, p = stats.mannwhitneyu(virginica, versicolor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db28345",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat,p # with a p_value of near-zero...there is evidence to suggest that a diff exists between the mean \n",
    "# petal_width for virginica and versicolor, ie the samples were drawn from seperate distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afdafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare virginica and versicolor mean PETL_LENGTH using Mann-Whitney\n",
    "virginica = train_iris[train_iris.species=='virginica'].petal_length\n",
    "versicolor = train_iris[train_iris.species=='versicolor'].petal_length\n",
    "stat, p = stats.mannwhitneyu(virginica, versicolor)\n",
    "stat,p # with a p_value of near-zero...there is evidence to suggest that a diff exists between the mean \n",
    "# petal_length for virginica and versicolor, ie the samples were drawn from seperate distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a68cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare virginica and versicolor mean SEPAL_WIDTH using Mann-Whitney\n",
    "virginica = train_iris[train_iris.species=='virginica'].sepal_width\n",
    "versicolor = train_iris[train_iris.species=='versicolor'].sepal_width\n",
    "stat, p = stats.mannwhitneyu(virginica, versicolor)\n",
    "stat,p # with a p_value less than alpha (0.05)...there is evidence to suggest that a diff exists between\n",
    "# the mean sepl_width for virginica and versicolor, ie the samples were likely drawn from seperate distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e25979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare virginica and versicolor mean SEPAL_LENGTH using Mann-Whitney\n",
    "virginica = train_iris[train_iris.species=='virginica'].sepal_length\n",
    "versicolor = train_iris[train_iris.species=='versicolor'].sepal_length\n",
    "stat, p = stats.mannwhitneyu(virginica, versicolor)\n",
    "stat,p # with a p_value less than alpha (0.05)...there is evidence to suggest that a diff exists between\n",
    "# the mean sepl_length for virginica and versicolor, ie the samples were likely drawn from seperate distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061315cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' all pair-wise comparisons display a statistically significant diff in the means of each measure for\n",
    "virginica and versicolor.  In particular, the petal_width and petal_length means-testing display STRONG EVIDENCE\n",
    "for the conclusion that the measurements are sourced from seperate dirstributions.\n",
    "Regarding sepal_width and sepal_length, while the p_value is below the alpha threshold (0.05), the slight possibility\n",
    "exists in the case of sepal_width of a Type II error (approx 4%).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOK at MUltivariate comparisons, using pairplot\n",
    "sns.pairplot(train_iris,corner=True,hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interaction of two diff numeric variables and represent the SPECIES dimension with hue\n",
    "# I will look  at sepal_length and sepal_width because the three centroids (species) appear to be closest\n",
    "sns.relplot(data=train_iris,\n",
    "              x= 'sepal_length',\n",
    "              y= 'sepal_width',\n",
    "              hue='species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4551b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf14c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to melt this WIDE table into a LONG table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fdf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris_melt = train_iris.melt(id_vars=['species'],\n",
    "                                  var_name='category',\n",
    "                                  value_name='value'\n",
    "                                  )\n",
    "train_iris_melt\n",
    "# 84 instances X 4 attributes = 336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4177d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create SWARMPLOT x=category y=value hue=species\n",
    "sns.swarmplot(data=train_iris_melt,\n",
    "              x= 'category',\n",
    "              y= 'value',\n",
    "              hue='species')\n",
    "\n",
    "#That is awesome! So cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9670e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The SWARMPLOT based on a LONG table clearly displays instances where variables are distinct and seperate from\n",
    "one another based upon dimension of SPECIES.  Likewise it also shows where distributions overlap.  This can be \n",
    "used to pinpoint those variables which may prove to be useful drivers/predictors.\n",
    "\n",
    "In this case, as was shown with earlier analysis...Petal_Length and Petal_Width both display clear and distinct\n",
    "1-dimensional clusters with minimal overlap of data points\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb303bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOVEL QUESTION... Is Sepal_Width of Virginica statistically significant in size from\n",
    "# VERSICOLOR samples (2-sample,2-tailed)? \n",
    "\n",
    "# First we plot\n",
    "sns.boxplot(data=train_iris,x='species', y='sepal_width')\n",
    "plt.axhline(train_iris['sepal_width'].mean(),color=\"red\", linestyle=\"dashed\",linewidth=1)\n",
    "plt.title(f'species by Sepal_Width (redline is mean of ALL species)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_α = 0.05\n",
    "vir_sepw = train_iris_melt[(train_iris_melt['species'] == 'virginica') & (train_iris_melt['category'] == 'sepal_width')].value\n",
    "versi_sepw = train_iris_melt[(train_iris_melt['species'] == 'versicolor') & (train_iris_melt['category'] == 'sepal_width')].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e30bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we run a ttest with assumptions of normality, independence, and variance met\n",
    "_, levene_p = stats.levene(vir_sepw,versi_sepw)\n",
    "levene_p < check_α\n",
    "#levene_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3462437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing both Virginica and Versicolor SEPAL WIDTH via a ttest_ind(2-sample), (2-tailed)\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(vir_sepw,\n",
    "                                versi_sepw,\n",
    "                               equal_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stat,p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6667ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "While looking at the boxplot of the various species MEAN SEPAL_WIDTH may not indicate a statistically \n",
    "significant difference between VIRG and VERSI, further ttest_ind provieds evidence that a \n",
    "Statistically Sig Diff does exists between the two.\n",
    "\n",
    "This is an excellent way to illustrate the point that the SCALE of the CHART may not always display the full\n",
    "nature of the truth.  By incorporating the SETOSA data in the Boxplot, it skews the relative view of the two \n",
    "species under investigation and may lead to the viewer making a false negative when eyeballing a chart that \n",
    "is not constructed optimally!\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82377ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7486a43",
   "metadata": {},
   "source": [
    "## PART II using Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load titanic via acquire.py\n",
    "df = acq.get_titanic_data('titanic_db')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prp.prep_titanic(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5cb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34815d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train, validate, test = prp.split_data(df,'survived')\n",
    "train.shape,validate.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc268dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine drivers of target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22424abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols, num_cols = [], []\n",
    "for col in train.columns[:-2]:\n",
    "    if train[col].dtype == 'O':\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        if train[col].nunique() < 10:\n",
    "            cat_cols.append(col)\n",
    "        else:\n",
    "            num_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc602e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_cols = cat_cols + num_cols\n",
    "explore_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Univariate Counts of Survival status by each column:\n",
    "for col in explore_cols:\n",
    "    if col in cat_cols:\n",
    "        print(f'Univariate assessment of feature {col}:')\n",
    "        sns.countplot(data=train, x=col)\n",
    "        plt.show()\n",
    "        print(\n",
    "            pd.concat([train[col].value_counts(),\n",
    "            train[col].value_counts(normalize=True)],\n",
    "                 axis=1))\n",
    "    else:\n",
    "        print(f'Univariate Feature analysis of feature {col}:')\n",
    "        plt.hist(train[col])\n",
    "        plt.show()\n",
    "        sns.boxplot(data=train, x=col)\n",
    "        plt.show()\n",
    "        train[col].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8779ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate analysis\n",
    "for col in cat_cols:\n",
    "    if col != 'survived':\n",
    "        sns.barplot(data=train,\n",
    "                    x=col,\n",
    "                    y = 'survived',\n",
    "                   ci=False)\n",
    "        plt.title(f'Surivival of Passengers based on {col}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ada6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drivers of signifigance include:\n",
    "#  pclass,gender, and alone (or sibsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4996f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to potentially drop include:\n",
    "#  sibsp and parch-assuming data integrity, then alone = 1 | 0 IS SUFFICIENT\n",
    "#  unless specific domain knowledge says otherwise....drop Embark Town\n",
    "#  sex is redundancy of sex_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf58e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns to potentially bin as discretes:\n",
    "#  both age and fare are potnetial candidates for binning...B/C this is Classification and not regression,\n",
    "# we are not looking to determine or even use precice, exact values, we just need to know categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0189276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to potentially combine:\n",
    "# if two or more columns communicate similar domain info, rather than combine them, just drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79225e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Not that it is surprising, but for categorical dimension except sibsp and parch, the feature class that\n",
    "contained the largest number of instances also lead to the correspndingly lowest survival rate within\n",
    "the feature.  For example, survival rate for 3rd class pax was lowest, but they constituted largest portion\n",
    "of pax, survival rate for male was lower than female, and they also constituted largest portion of pax\n",
    "within feature. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88025b2",
   "metadata": {},
   "source": [
    "## USE TELCO dataset look for drivers of CHURN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0647c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acq.get_telco_data('telco_churn')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771538c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather than use prep_telco fx from prepare.py, I will be proceed manually, b/c analysis requires \n",
    "# different df transformations than prepare.py fx. I do not want to encode any of the values thereby making\n",
    "# table unneccesarily wider. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b222635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['payment_type_id','internet_service_type_id','contract_type_id',\n",
    "                     'customer_id','payment_type'])\n",
    "df\n",
    "\n",
    "# droppage for most of the columns that provide duplicated info...I left the human-friendly str versions\n",
    "# and removed the machine-friendly numerically encoded versions\n",
    "#  additionally, customer_id goes bye-bye, then I also decided to just remove\n",
    "# payment_type altogether.  But now I dont remember why..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93876f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any numerical value to BIN?  \n",
    "# again, this is a classification problem, so there is no harm in placing monthly charge and total_charge\n",
    "# into tiers.  Maybe some ML algorythms may perform better with continuous data? prolly not tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather than combine similar features, I would look at opportunities for dimension reducation instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7b7b1",
   "metadata": {},
   "source": [
    "As a result of having analyized churn in a previous module, via Tableau, I will pare the df down to only those\n",
    "features which indicate whether or not a customer is tied to TelcoCo in some non-business, ancillary manner\n",
    "or if the cost for a customer to switch service providers is nominally nothing.\n",
    "\n",
    "For example, having one or more dependents who are also serviced by TelcoCo keeps an accnt from churning\n",
    "b/c the switching costs for those accounts may be greater than some single dude on a month-to-month contrct.\n",
    "\n",
    "The greater the perceived commitment...the gretaer the length of tenure.\n",
    "\n",
    "Significant drivers of churn: dependents-paperless_billing-contract_type\n",
    "Largest CUSTOMER SEGMENT  contract_type=Month-to-Month\n",
    "                          paperless_billing=enabled\n",
    "                          dependents=No\n",
    "                          Service_Type=Fiber_optic\n",
    "                          1420 accounts\n",
    "Churn, Baby, Churn!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pydataset import data\n",
    "import env\n",
    "import acquire as acq\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# query = define the query above the function call and pass the variable in as a parameter\n",
    "fruits_df = acq.get_sql_pull('fruits_db',\n",
    "                \"SELECT * FROM fruits\")\n",
    "\n",
    "fruits_df\n",
    "\n",
    "fruits_df.info()\n",
    "\n",
    "import prepare as prp\n",
    "\n",
    "cat_cols,num_cols = prp.cat_num_cols(fruits_df)\n",
    "\n",
    "cat_cols,num_cols\n",
    "\n",
    "prp.remind_data_prep()\n",
    "\n",
    "prp.ex_cat_cols(cat_cols,fruits_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2497572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
